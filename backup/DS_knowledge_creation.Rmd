---
title: "Data Science Knowledge Creation (In-Class Response)"
author: Emilio Luz-Ricca
output:
  html_document:
    toc: true
    number_sections: true
    highlights: pygments
---

<!-- 
Outline:
  -Point to be made: advances in data collection are pushing us closer to robust modeling techniques that can service the regions that need them most (i.e., global south and LMICS)
    -Moving away from producing models trained on HIC data is important here
    -In this era of big data which is continuously collected on everyone, statistical disclosure, privacy, etc. are increasingly of concern and must CONSTANTLY be kept in mind
  -Secondary point: introducing novel modeling techniques, as always, drives innovation... we can't just rely on legacy systems or outdated techniques
    -Multidisciplinary techniques are especially important here (i.e., gravity model which uses physics in human geography)
  -To conclude: we are truly in the era of big data and data science techniques are incredibly important... still, we must figure out how to continue to push the envelope in order to produce results that are targeted at populations that need them most, while still respecting privacy and individuality
-->

We clearly are living in the era of big data. With novel data sources comes novel techniques, which broadly fall under this multidisciplinary field we call data science. Certainly, we have come a long way in describing human processes and we have even begun to decode elements such as scale and complexity. However, growing data availability also brings along concerns of privacy, especially when dealing with sensitive information. So, as data scientists we must balance the demand for innovation with moral and ethical concerns, ultimately producing models that are compassionate and aware of the context in which they were created (less in an artificial intelligence sense, and more so in the sense of human individuality). We must continue to produce models that are accurate, spatially realistic, and fine in granularity, with the goal of progressing not only our understanding of human systems, but also understanding the more general implications of our results.

As is evident to anyone alive today, data is a resource in abundance: our data is constantly harvested as we browse the internet, satellites routinely collect earth observation data, and so on. However, I argue that this data abundance is deceptive. How so? Data of course is an asset and can be quite expensive to collect or maintain. So, while there is an abundance of data, this data is often inaccessible to those within the public sector, research, or academia. A factor that has been vital in the recent academic advances in data science has been the innovation and use of _alternate_ forms of data. This might include remotely-sensed, crowd-sourced, or entirely novel forms of data. This past semester, I wrote a Data Science Insight on using natural language processing (NLP) techniques to geolocate Tweets. This is a perfect example of an alternative data source: we can essentially refine an abundant text data source to mine location data. While perhaps this particular form of data mining is not as useful within the context of development, it is this same kind of innovation that is absolutely necessary when approaching problems within development work. Another example, which is much more relevant to development, is the use of nighttime lights spatial data as a proxy for GDP and an indicator of general development within a region. Satellite imagery is increasingly being released for public research by fantastic organizations like the Radiant Earth Foundation and WorldPop, which increases the feasibility of these kinds of approaches. As always, there is a strong  need for creativity and innovation in data production. Data provides the basis for an analysis, and improving the data used often greatly improves results.

While forming these novel techniques for data collection is exciting, there are also valid concerns over privacy. Certainly, being able to infer location from a tweet is incredibly powerful, but could be quite dangerous if abused. Thus, there must always be a certain amount of caution when applying these techniques. Similarly, gathering census data allows us to both describe a population and make useful predictions (i.e., land use, movement, etc.). But, as in the previous example, citizens in LMICs have valid concerns over the uses of the data collected (which can be particularly concerning in countries with less stable governments). Does this mean we shouldn't try to collect this data? Probably not: inferences made based on accurate data can help guide policy and decision-making processes in a more informed, more realistic manner. So, what is the solution? One solution is to produce synthetic data sets, which are based off of accurate data sources but increase observations through a number of different techniques. For instance, I worked to produce a realistic synthetic population of Eswatini this semester using DHS survey data. Using machine learning and statistical techniques, I was able to expand trends in the relatively small survey (around 5000 observations) to the entire population (over one million people). When producing synthetic data sets, we must be aware of sources of error (in my analysis, for instance, I did not meaningfully use the survey weights, which was a source of error). However, synthetic data sets are a great way to expand a base data set while still respecting elements such as individual privacy and statistical disclosure limitations.

Once we have collected (or produced) our data set, we can finally move on to modeling and making predictions. While numerous advances have been made recently in terms of modeling human behavior, it is my opinion that further innovations in this context will require multidisciplinary approaches. As an example, the gravity model for migration bases off of a concept within physics, but has displayed real predictive power for many aspects in human behavior. I believe that further advances in data science, and the social sciences more generally, will require more approaches that draw from several fields at once. Additionally, we must work to adopt new techniques and new modeling approaches. If we had decided to only used regression analysis using classic statistical techniques for prediction, we would never have made the phenomenal gains in predictive power that we have seen recently. Innovation in modeling is most important for the advances in the application of the models, indicating a constant need for re-evaluation of techniques. If our goal (very broadly) is to help people, we can only do that if we leverage the best possible data with the most accurate models. If we can do that, all that is left is to present findings in an accessible manner that allows even those with very little or no background in the field (i.e., politicians) to understand the results.

I have seen throughout this course how important multidisciplinary collaboration can be in efforts within data science. Agent based modeling describes a class of models, but more broadly represents efforts to understand complex systems with emergent properties. Only through the use of novel data sources, collaboration across fields, creativity, and a willingness to adapt to new techniques can we truly hope to describe human processes and begin to understand outcomes of human behavior in the aggregate.


